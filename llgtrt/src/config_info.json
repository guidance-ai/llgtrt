{
 "##info##": "Use scripts/regen.sh to re-generate this file",
 "runtime": {
  "#": "TensorRT-LLM runtime parameters\nDefaults should be reasonable, otherwise see\nhttps://nvidia.github.io/TensorRT-LLM/performance/perf-best-practices.html",
  "guaranteed_no_evict": {
   "#": "Make the scheduler more conservative, so that a started request is never evicted.\nDefaults to false (which improves throughput)"
  },
  "max_batch_size": {
   "#": "Maximum number of concurrent requests"
  },
  "max_num_tokens": {
   "#": "Maximum number of tokens in batch"
  },
  "max_queue_size": {
   "#": "Maximum number of requests in queue (when batch already full)"
  },
  "enable_chunked_context": {
   "#": "Chunk prefill/generation into pieces\nDefaults to true (unlike trtllm)"
  },
  "enable_kv_cache_reuse": {
   "#": "Prefix-caching (LRU-reuse blocks between requests)\nDefaults to true (unlike trtllm)"
  },
  "kv_cache_free_gpu_mem_fraction": {
   "#": "Fraction of free GPU memory to use for KV cache"
  },
  "kv_cache_host_memory_megabytes": {
   "#": "Host memory to use for KV cache"
  },
  "kv_cache_onboard_blocks": {
   "#": "Controls whether offloaded blocks should be onboarded back into primary memory before being reused.\nDefaults to true."
  },
  "cross_kv_cache_fraction": {
   "#": "The fraction of the KV Cache memory should be reserved for cross attention\nIf set to p, self attention will use 1-p of KV Cache memory and cross attention\nwill use p of KV Cache memory.\nShould only be set when using encoder-decoder model."
  },
  "secondary_offload_min_priority": {
   "#": "Only blocks with priority > mSecondaryOfflineMinPriority can be offloaded to secondary memory."
  },
  "event_buffer_max_size": {
   "#": "Max size of the KV cache event buffer"
  },
  "enable_batch_size_tuning": {
   "#": "Control automatic tuning of batch size\nDefaults to true (unlike trtllm)"
  },
  "enable_max_num_tokens_tuning": {
   "#": "Control automatic tuning of max num tokens\nDefaults to true (unlike trtllm)"
  }
 },
 "tokenizer": {
  "#": "Tokenizer configuration (defaults to tokenizer_config.json contents)\nTypically no changes are needed here, except for chat_template\nwhich is best overridden with --chat-template filename.j2 option.",
  "json_start_token": {
   "#": "This is <|python_tag|> for Llama 3 models."
  },
  "n_vocab_override": {
   "#": "Use to override tokenizer vocabulary size.\nUse 32064 for phi3."
  }
 },
 "llguidance": {
  "#": "Configuration for the LLGuidance constraint library",
  "limits": {
   "#": "Override any of the parser limits.",
   "max_items_in_row": {
    "#": "For non-ambiguous grammars, this is the maximum \"branching factor\" of the grammar.\nFor ambiguous grammars, this might get hit much quicker.\nDefault: 2000"
   },
   "initial_lexer_fuel": {
    "#": "How much \"fuel\" are we willing to spend to build initial lexer regex AST nodes.\nDefault: 1_000_000\nSpeed: 50k/ms"
   },
   "step_lexer_fuel": {
    "#": "Maximum lexer fuel for computation of the whole token mask.\nDefault: 200_000\nSpeed: 14k/ms"
   },
   "step_max_items": {
    "#": "Number of Earley items created for the whole token mask.\nDefault: 50_000\nSpeed: 20k/ms"
   },
   "max_lexer_states": {
    "#": "Maximum number of lexer states.\nDefault: 50_000"
   },
   "max_grammar_size": {
    "#": "Maximum size of the grammar (symbols in productions)\nDefault: 500_000 (a few megabytes of JSON)"
   }
  },
  "log_level": {
   "#": "Log level which goes to stderr. In-memory logs per-sequence are managed by ConstraintInit.log_level."
  }
 },
 "py": {
  "#": "Configuration for the embedded Python API",
  "input_processor": {
   "#": "Path to a Python script that does multi-modal and chat template processing\nDefaults to <engine>/input_processor.py if it exists"
  },
  "hf_model_dir": {
   "#": "Path to HuggingFace model directory; defaults to engine directory"
  },
  "visual_engine_dir": {
   "#": "Path to TRT-LLM visual engine; defaults to <engine>/visual_engine"
  },
  "arguments": {
   "#": "Additional arguments passed to the Python script"
  }
 }
}